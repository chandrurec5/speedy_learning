%!TEX root =  speedylearn.tex
\section{Temporal Difference Learning Algorithms}
\subsection{Noisy Discretization Schemes}
The idea of using ODEs is not restricted to just analysis, and can extend to synthesis, i.e., new SA algorithms can be obtained as noisy discretization of ODEs (which converges to the desired solution). Discretization is a common theme in numerical solution methods to solve ODEs, and of particular interest are the Euler discretization and the \emph{predictor-corrector} discretization schemes. Given an ODE $\dot{x}(t)=(g-Hx(t))$, the noisy Euler discretization is given in \eqref{linearrec} and the noisy PC discretization is given by
\begin{align}\label{PC}
\begin{split}
x^m_{t}&=x_t+\alpha_t(g-Hx_t)\\
x_{t+1}&=x_t+\alpha_t(g-Hx^m_t)
\end{split}
\end{align}
The idea behind the PC method is to first take a step in the gradient direction to produce a new point $x^m_t$, and then obtain the estimate of the gradient at $x^m_t$ to be used to update $x_t$. Notice that the PC updates can be unfurled and written as the following single recursion
\begin{align*}
\begin{split}
x_{t+1}&=x_t+\alpha_t(g-H(x_t+\alpha_t(g-Hx_t)))\\
&=x_t+\alpha_t(g+\alpha_t g- H +\alpha_t H^2)x_t \\
&= (I-\alpha_t H+\alpha_t^2 H^2)x_t+\alpha_t(g-\alpha_t H g)
\end{split}
\end{align*}
The \emph{temporal difference} (TD) class of learning algorithms solve the projected Bellman equation. Since in the RL setting, $b_\pi$ and $A_\pi$ are not known explicitly, the TD algorithms perform updates based on noisy observations, and are hence LSA algorithms as well. The different designs of the TD algorithms are meant to address the different issues such as stability and speed of convergence.
While the design changes (primary/composite) across the different TD variants, a common feature amongst almost all the TD algorithms is the use of the temporal difference which is given by
\begin{align}\label{td}
\delta_t(\theta)=R(s_t,a_t)+\gamma \phi'_t\theta-\phi_t\theta,
\end{align}
where $\phi_t=\phi(s_t)^\top \in \R^n$ is the feature of state $s_t$ and is the $s^{th}$ row of the feature matrix $\Phi$ and in a similar way $\phi'_t=\phi(s'_t)$. Thus $\delta_t$ is the error in the current estimate of the value function.\par
The simplest of the TD family namely TD($0$) is given by,
\begin{align}\label{tdzero}
&\textbf{\tdo:}&\theta_{t+1}=\theta_t+\alpha_t\delta_t(\theta_t)\phi_t^\top,
\end{align}
It is clear that $TD(0)$ is the following LSA algorithm in $n$ variables
\begin{align}\label{tdzero}
\theta_{t+1}=\theta_t+\alpha_t(b_\pi-A_\pi)\theta_t+\alpha_t M_{t+1},
\end{align} 
with design $\langle b_\pi,A_\pi \rangle$. In order to arrive at \eqref{tdzero} we have made use of the fact that $s_t\sim D_\pi, a_t\sim\pi(s_t,\cdot)$, $\mathbf{E}[\delta_t(\theta)|s_t]= \big(R_\pi(s_t)+\gamma (P_\pi J_\theta)(s_t) -J_\theta(s_t)\big)$ and letting $M_{t+1}=\delta_t\phi_t^\top-\mathbf{E}[\delta_t\phi_t^\top]$.\par
The TD($0$) algorithm can exhibit divergent behaviour in the \emph{off-policy} setting. In the \emph{off-policy} setting the recursion in \eqref{tdzero} has to be modified as follows
\begin{align}\label{tdzerooff}
\theta_{t+1}=\theta_t+\alpha_t\rho_t(s_t,a_t)\delta_t\phi_t^\top,
\end{align}
where $\rho_t\stackrel{\cdot}{=}\rho(s_t,a_t)$ is the importance sampling ratio between the behaviour policy $\mu$ and target policy $\pi$. The recursion in \eqref{tdzerooff} can be re-written as
\begin{align}\label{tdzerooffsa}
\theta_{t+1}=\theta_t+\alpha_t(b_\pi-A^\mu_\pi)\theta_t+\alpha_t M_{t+1},
\end{align}
where $A^{\mu}_\pi=\Phi^\top D_\mu (I-\gamma P_\pi)\Phi$ , where $\mu$ is the behaviour policy used to sample the data. Thus the design of \tdo can in the off-policy case is $\{b_\pi,A^\mu_\pi\}$.
The undoing of $TD(0)$ in the off-policy is due to the fact $A^\mu_\pi$ cannot be guaranteed to be positive definite. This condition can be demonstrated via the following simple example from \cite{etd}.
\begin{example}
Consider a two state MDP with $P_\pi=
\begin{bmatrix}
    0       & 1  \\
    0       & 1 \\
\end{bmatrix}$, $\Phi=\begin{bmatrix}
 1  \\
2 \\
\end{bmatrix}$, $D_\mu=
\begin{bmatrix}
    0.5       & 0  \\
    0       &0.5 \\
\end{bmatrix}$. Then the design matrix $A^\mu_\pi=\Phi^\top(I-\gamma P_\pi)\Phi=-0.2$ is negative definite and hence from \Cref{linstab} it follows that \tdo is divergent for this example.
\end{example}
%\input{offpol}
\subsection{Gradient Temporal Difference Learning}
The instability of TD($0$) is due to the fact that it is not a true gradient descent algorithm. The first gradient-TD (GTD) algorithm was proposed by \citet{sutton2009convergent} and is based on minimizing the \emph{norm of the expected TD update} (NEU) given by
\begin{align}\label{neu}
NEU(\theta)=\parallel b_\pi -A_\pi\theta\parallel^2
=\E[\rho_t\phi_t^\top\delta_t(\theta)]^\top\E[\rho_t\phi_t^\top\delta_t(\theta)]
\end{align}
The GTD scheme based is on the gradient of the above expression which is given by (dropping subscript $t$ for convenience) $-\frac{1}{2}\nabla NEU(\theta)=\E[\rho (\phi-\gamma\phi')^\top \phi]\E[\rho \phi^\top\delta(\theta)]$. Since the gradient is a product of two expectation we cannot use a sample product (due to the presence of correlations). The GTD addresses this issue by estimating $\E[\rho\delta\phi^\top]$ in a separate recursion. The GTD updates can be given by
\begin{align}
\begin{split}
\textbf{GTD:\quad}y_{t+1}&=y_t+\alpha_t(\rho_t\phi^\top\delta_t -y_t)\\
\theta_{t+1}&=\theta_t+\alpha_t\rho_t(\phi_t-\gamma\phi'_t)^\top\phi_ty_t
\end{split}
\end{align}
Notice that $y$ updates are noisy Euler discretization of the ODE $\dot{y}=\E[\rho\delta\phi^\top]-y(t)$. The overall design of GTD is given by $\D_{GTD}=\langle \begin{bmatrix}b_\pi\\ 0\\ \end{bmatrix},\begin{bmatrix}-I &-A^\mu_\pi \\ {A^\mu_\pi}^\top &0 \\ \end{bmatrix}\rangle$.\par
%$\{g_1,H_1\}$ where $g_1=\begin{bmatrix}b_\pi\\ 0\\ \end{bmatrix}$, $H_1=\begin{bmatrix}-I &-\Phi^\top D_\mu (\Phi -\gamma P_\pi\Phi) \\ (\Phi -\gamma P_\pi\Phi)^\top D_\mu\Phi &0 \\ \end{bmatrix}$.\par
Instead of NEU, the \emph{mean-square projected Bellman Error} (MSPBE) can also be minimized. The MSPBE is defined as
\begin{align}\label{mspbe}
MSPBE(\theta)=\parallel J_\theta-\Pi T_\pi J_\theta \parallel^2_D
\end{align}
The GTD2 algorithm was proposed in \cite{gtdref} based on minimizing \eqref{mspbe}. The GTD2 updates are given by
\begin{align}
\begin{split}
\textbf{GTD2:\quad}y_{t+1}&=y_t+\beta_t\phi_t^\top(\rho_t\delta_t-\phi_t y_t)\\
\theta_{t+1}&=\theta_t+\alpha_t\rho_t(\phi_t-\gamma\phi’_t)^\top\phi_t y_t
\end{split}
\end{align}
The design of GTD2 is given by $\D_{GTD2}=\langle \begin{bmatrix}b_\pi\\ 0\\ \end{bmatrix}, \begin{bmatrix}-M &-A^\mu_\pi \\ {A^\mu_\pi}^\top &0 \\ \end{bmatrix}\rangle$, where $M=\Phi^\top D_\mu\Phi$.\par
The GTD-\emph{Mirror Prox}(GTD-MP) algorithm is given by the following update rule:
\begin{align}\label{gtdmp}
\begin{split}
\textbf{GTD-MP:} y_t^m=y_t+\alpha_t{\phi_t}^\top(R(s_t,a_t)+\gamma (\phi'_t-\phi_t)\theta_t),\\ \theta_t^m=\theta_t+\alpha_t({\phi_t}-\gamma\phi'_t )^\top\phi_ty_t,\\
 y_{t+1}=y_t+\alpha_t{\phi_t}^\top(R(s_t,a_t)+\gamma(\phi'_t-\phi_t)\theta^m_t), \\ \theta_{t+1}=\theta_t+\alpha_t({\phi_t}-\gamma\phi'_t )^\top\phi_ty^m_t,
\end{split}
\end{align}
The GTD-MP algorithm in \eqref{gtdmp} is the PC discretization of the GTD algorithm with the design matrix as $\D_{GTD-MP}=\langle (I-\alpha_t H_{GTD})g_{GTD}, H_{GTD}-\alpha_t H^2_{GTD}\rangle$. In a similar fashion, one can derive the GTD2-MP algorithm as the PC discretization of GTD2 algorithm.\par
\begin{comment}
\subsection{Speedy Learning}
Speedy Q-learning was introduced in \cite{sqlpaper} and uses a `speed-up' term to achieve better convergence rates. The SQL updates for the full state representation is given by
\begin{align}\label{sql}
\begin{split}
&\textbf{SQL:\quad}\delta_t=R(s_t,a_t)+\gamma J_{t-1}(s_{t+1})-J_{t}(s_t),\\
&\Delta_t= \big((R(s_t,a_t)+\gamma J_t(s_{t+1}))-(R(s_t,a_t)+\gamma J_{t-1}(s_{t+1}))\big)\\&J_{t+1}(s)=J_t(s)+\alpha_t\delta_t+(1-\alpha_t)\Delta_t
\end{split}
\end{align}
Note that a speed-term $\Delta_t$ is added to the temporal difference term $\delta_t$. Also, the step size accompanying the speed up term is `aggressive’ in that it is $(1-\alpha_t)\approx 1$ for small $\alpha_t$. The ODE associated with SQL is given by
\begin{align}
\begin{split}
\dot{J_\pi}(t)=(R_\pi+\gamma P_\pi J_\pi(t) -J_\pi(t))+\gamma P_\pi\dot{J_\pi}(t)
\dot{J_\pi}(t)=(I-\gamma P_\pi)^{-1}R_\pi-J_\pi(t))
\end{split}
\end{align}
The design corresponding to SQL is $\D_{SQL}=\langle(I-\gamma P_\pi)^{-1}R_\pi,I\rangle$.
\subsection{Synthesis of New algorithms}
Consider the following RL algorithm called SQLFA which combines SQL and function approximation for the purpose of on-policy evaluation:
\begin{align}\label{sqlfa}
\begin{split}
&\textbf{SQLFA:\quad}\theta_{t+1}=\theta_t+\phi(s_t)^\top \alpha_t\big(R(s_t,a_t)+\gamma \phi(s_{t+1})\theta_{t-1}\\&-\phi(s_t)\theta_t +(1-\alpha_t)(\gamma \phi(s_{t+1})\theta_t-\gamma \phi(s_{t+1})\theta_{t-1}\big)
\end{split}
\end{align}
The ODE corresponding to SQLFA is given by
\begin{align}
\begin{split}
&\dot{\theta}(t)=(b_\pi-A_\pi\theta_t)+\gamma\Phi^\top D_\pi P_\pi\Phi^\top\dot{\theta}(t)\\
&\dot{\theta}(t)=(I-\gamma\Phi^\top D_\pi P_\pi\Phi)^{-1}(b_\pi-A_\pi\theta_t)
\end{split}
\end{align}
\begin{align}
\begin{split}
\theta_t&=\theta_t+\alpha_t(y_t-\phi(s_t)^\top(\phi(s_t)\theta_t-\gamma \phi(s_{t+1})\theta_t))\\
y_{t+1}&=y_{t}+\alpha_t(\delta_t)
\end{split}
\end{align}
\end{comment}