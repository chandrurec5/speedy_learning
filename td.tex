\section{Temporal Difference Learning Algorithms}
The \emph{temporal difference} (TD) class of learning algorithms solve the projected Bellman equation. The simples of the TD family namely TD($0$) is given by,
\begin{align}\label{tdzero}
\theta_{t+1}=\theta_t+\alpha_t(\underbrace{R(s_t,a_t)+\gamma \phi(s_{t+1})\theta_t-\phi(s_t)\theta_t}_{{\delta_t}}){\phi(s_t)}^\top,
\end{align}
where $\phi(s_t)^\top \in \R^n$ is the feature of state $s_t$ and is the $s^{th}$ row of the feature matrix $\Phi$. In \eqref{tdzero}, $\delta_t$ is the temporal difference term which is an estimate of the error in the value function specified by $\theta_t$. The $TD(0)$ update can also be re-written as
\begin{align}\label{tdzero}
\theta_{t+1}=\theta_t+\alpha_t(b_\pi-A_\pi)\theta_t+\alpha_t M_{t+1}
\end{align}
where $b_\pi=\Phi^\top D_\pi R_\pi$ and $A_\pi=\Phi^\top D_\pi (I-\gamma P_\pi)\Phi$. In order to arrive at \eqref{tdzero} we have made use of the fact that $s_t\sim D_\pi, a_t\sim\pi(s_t,\cdot)$, $\mathbf{E}[\delta_t|s_t]= \big(R_\pi(s_t)+\gamma (P_\pi J_\theta)(s_t) -J_\theta(s_t)\big)$ and letting $M_{t+1}=\delta_t\phi(s_t)^\top-\mathbf{E}[\delta_t{\phi(s_t)}^\top]$.\par
Note that the design of TD($0$) is given by $\{b_\pi,A_\pi\}$.
\subsection{Off-policy divergence of TD($0$)}
The TD($0$) algorithm can exhibit divergent behaviour in the \emph{off-policy} setting. In the \emph{off-policy} setting the recursion in \eqref{tdzero} has to be modified as follows
\begin{align}\label{tdzerooff}
\theta_{t+1}=\theta_t+\alpha_t(\rho_t(s_t,a_t){R(s_t,a_t)+\gamma \phi(s_{t+1})\theta_t-\phi(s_t)\theta_t}){\phi(s_t)}^\top,
\end{align}
where $\rho_t\stackrel{\cdot}{=}\frac{\pi(s_t,a_t)}{\mu(s_t,a_t)}$ is the importance sampling ratio between the behaviour policy $\mu$ and target policy $\pi$. The recursion in \eqref{tdzerooff} can be re-written as
\begin{align}\label{tdzerooffsa}
\theta_{t+1}=\theta_t+\alpha_t(b_\pi-A^\mu_\pi)\theta_t+\alpha_t M_{t+1},
\end{align}
where $A^{\mu}_\pi=\Phi^\top D_\mu (I-\gamma P_\pi)\Phi$ , where $\mu$ is the behaviour policy used to sample the data. Thus the design of \tdo can in the off-policy case is $\{b_\pi,A^\mu_\pi\}$.
The undoing of $TD(0)$ in the off-policy is due to the fact that all the eigen values of $A^\mu_\pi$ cannot be guaranteed to have positive real parts. This condition can be demonstrated via the following simple example from \cite{}.
\begin{example}
Consider a two state MDP with $P_\pi=
\begin{bmatrix}
    0       & 1  \\
    0       & 1 \\
\end{bmatrix}$, $\Phi=\begin{bmatrix}
 1  \\
2 \\
\end{bmatrix}$, $D_\mu=
\begin{bmatrix}
    0.5       & 0  \\
    0       &0.5 \\
\end{bmatrix}$. Then the design matrix $A^\mu_\pi=\Phi^\top(I-\gamma P_\pi)\Phi=-0.2$ is negative definite and hence from \Cref{linstab} it follows that TD($0$) is divergent for this example.

\end{example}
%\input{offpol}
\subsection{Gradient Temporal Difference Learning}
The instability of TD($0$) is due to the fact that it is not a true gradient descent algorithm. The first gradient-TD (GTD) algorithm was proposed in \cite{} and is based on minimizing the \emph{norm of the expected TD update} (NEU) given by
\begin{align}\label{neu}
NEU(\theta)=\E[\rho\phi^\top\delta(\theta)]\E[\rho\phi^\top\delta(\theta)]
\end{align}
The GTD scheme based is on the gradient of the above expression which is given by $-\frac{1}{2}\nabla NEU(\theta)=\E[\rho (\phi-\gamma\phi’)^\top \phi]\E[\rho \phi^\top\delta(\theta)]$. Since the gradient is a product of two expectation we cannot use a sample product (due to the presence of correlations). The GTD addresses this issue by estimating $\E[\rho\delta\phi^\top]$ in a separate recursion. The GTD updates can be given by
\begin{align}
\begin{split}
y_{t+1}&=y_t+\alpha_t(\rho_t\phi^\top\delta_t -y_t)\\
\theta_{t+1}&=\theta_t+\alpha_t\rho_t(\phi(s_t)-\gamma\phi(s_{t+1}))^\top\phi(s_t)y_t
\end{split}
\end{align}
The design of GTD is given by $\{g_1,H_1\}$ where $g_1=\begin{bmatrix}b_\pi\\ 0\\ \end{bmatrix}$, $H_1=\begin{bmatrix}-I &-\Phi^\top D_\mu (\Phi -\gamma P_\pi\Phi) \\ (\Phi -\gamma P_\pi\Phi)^\top D_\mu\Phi &0 \\ \end{bmatrix}$.\par
Instead of NEU, the \emph{mean-square projected Bellman Error} (MSPBE) can also be minimized. The MSPBE is defined as
\begin{align}\label{mspbe}
MSPBE(\theta)=\parallel J_\theta-\Pi T_\pi J_\theta \parallel^2_D
\end{align}
The GTD2 algorithm was proposed in \cite{} based on minimizing \eqref{mspbe}. The GTD2 updates are given by
\begin{align}
\begin{split}
y_{t+1}&=y_t+\beta_t\phi(s_t)^\top(\rho_t\delta_t-\phi(s_t)y_t)\\
\theta_{t+1}&=\theta_t+\alpha_t\rho_t(\phi(s_t)-\gamma\phi(s_{t+1}))^\top\phi(s_t)y_t
\end{split}
\end{align}
The design of GTD2 is given by $\{g_2,H_2\}$, where $g_2=(b_\pi,0)$, $H_2==\begin{bmatrix}-\Phi^\top D_\mu\Phi &-\Phi^\top D_\mu (\Phi -\gamma P_\pi\Phi) \\ (\Phi -\gamma P_\pi\Phi)^\top D_\mu\Phi &0 \\ \end{bmatrix}$
%\subsection{Saddle Point Formulation}
\subsection{Speedy Learning}
Speedy Q-learning was introduced in \cite{} and uses a `speed-up’ term to achieve better convergence rates. The SQL updates for the full state representation is given by
\begin{align}\label{sql}
\begin{split}
J_{t+1}(s)=J_t(s)+\alpha_t\big(R(s_t,a_t)+\gamma J_{t-1}(s_{t+1})-J_{t}(s_t) \big)+\\(1-\alpha_t)\big((R(s_t,a_t)+\gamma J_t(s_{t+1}))-(R(s_t,a_t)+\gamma J_{t-1}(s_{t+1}))\big)
\end{split}
\end{align}
The above scheme is the Euler discretization of the following ODE
\begin{align}
\dot{J_\pi}(t)=(R_\pi+\gamma P_\pi J_\pi(t) -J_\pi(t))+\gamma P_\pi\dot{J_\pi}(t)
\end{align}
The design corresponding to SQL is $\{(I-\gamma P_\pi)^{-1}R_\pi,I\}$.