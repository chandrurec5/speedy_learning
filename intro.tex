\section{Introduction}
\textbf{Reinforcement Learning} is a paradigm where the learning algorithm/agent needs to learn via direct interaction with the environment. In most cases, the underlying environment can be modelled as a Markov Decision Process (MDP). At time any given time instant, the agent knows the state (described via the state variables) of the environment (i.e., the MDP) and has to choose an action from a given action set. The action selection mechanism of the agent is formally known as the policy, and each policy is associated with a value function, which is a mapping from the set of states to reals that specifies the value obtained by following a given policy starting from a given state. Typically, in order to improve a given policy, its value function needs to be computed first. Thus learning the value function of a given policy assumes centre stage.\par
\textbf{Algorithms} that learn the value function need to address two important issues namely lack of explicit model information (of the underlying MDP) and dimensionality of the state space. It is known that the value function of a policy can be computed by solving a $|S|\times|S|$ (where $|S|$ is the cardinality of the state space) linear system known as the Bellman equation (BE). However, the coefficients of the BE are described in terms of the model parameters, which are not known explicitly in an RL setting. Due to the \emph{curse-of-dimensionality} $|S|$ grows exponentially in the number of state variables. As a result, when $|S|$ is very large, it is infeasible to store, retrieve and compute the value for each and every state.\par
\textbf{Linear Function Approximation} (LFA) is a common dimensionality reduction approach employed to reduce the computational overhead when $|S|$ is large. In this approach, the value function is approximated by a linear combination of $n$ ($<<|S|$) basis functions. Once the basis is chosen, an approximate value function can computed by solving a reduced ($n\times n$) linear system known as the projected Bellman equation (PBE). It is important to note that the coefficients of the PBE are dependent not only on the basis functions but also on the model parameters. Thus, RL algorithms need to compute the PBE solution using only the samples obtained via direct interaction.\par
The temporal difference (TD) family of learning algorithms are a very important sub-class of RL algorithms and compute the solution to the PBE. The first TD algorithm (i.e., TD($0$)) was introduced in \cite{} and TD with LFA was analyzed in \cite{}. The TD family includes several variants such as TD such as TD($\lambda$), SARSA, Q-learning, LSTD, iLSTD and Gradient-TD (GTD) \cite{}. Aspects such as 
sample efficiency, computational complexity and stability have driven the course of development of these various TD algorithms. The issue with TD($0$) is that it makes use of only the current sample and is not sample efficient. This issue was alleviated by the LTSD by making use of all the data to estimate and solve an empirical PBE. Since LSTD involves matrix updates and inversions at each step its computational complexity is minimum $O(n^2)$. In contrast, since simple TD($0$) involves only $O(n)$ computations per step, it continues to be of interest in applications in which cannot afford $O(n^2)$ required by LSTD.\par
The TD, Q-learning and SARSA algorithms are not stable in the most general setting, especially the \emph{off-policy} case. Here, the policy (\emph{target}) whose value function needs to be  computed which is different from the policy (\emph{behaviour}) that was used to collect the samples. The instability arises due to the fact that TD, SARSA and Q-learning are not true gradient algorithms. The Gradient TD (GTD) family of algorithms perform updates in the gradient direction and are stable. Even though GTD is stable in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent algorithm with respect to its original objective \cite{}. Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation.\par
\textbf{Motivation} The focus of this paper is the TD family of algorithms and we now describe our motivation. Most TD algorithms (except the LSTD) perform incremental updates, and an important aspect that affects the speed of convergence is the choice of step size. While, there are adaptive step-size that helps in increasing rates of convergence, a clear cut step size rule is missing in literature. Even though all the TD variants deal with the PBE, the derivation of the various algorithms are rather ad-hoc \cite{}. As a result, there is no holistic framework that enables us to design stable and computationally efficient RL algorithms.\par
\textbf{Methodology} Most TD algorithms perform noisy updates to the weights using at time step, and hence under the category of stochastic approximation algorithms. The term stochastic approximation signifies the fact that the noisy quantities used in the algorithms are stochastically approximate, and are equal to the quantities only in the expectation. The SA oft theory considers the algorithm to exhibit an `averageâ€™ behaviour which is corrupted by noise. Under certain reasonable conditions, the noise can be discarded in the asymptotic limit and the average behaviour can then be shown to be well approximated by an ordinary differential equation corresponding to a given SA algorithm. The asymptotic behaviour of the algorithm can then studied by analyzing the ODE. In this paper, we make use of the theory of stochastic approximation algorithms to understand the TD algorithms in a holistic manner.\par
The specific contributions are listed as under.
\begin{itemize} 
\item We then argue on the lines of \cite{} and show that the finite time performance of any TD algorithm has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples). 
\item An interesting feature to note is that the bias term is affected by the condition number of the matrix involved in the PBE. This is in line with the asymptotic results in SA theory, i.e., the driving matrix of the ODE corresponding to the algorithm affects its asymptotic rate of convergence. 
\item We argue that the ill-effects of conditioning can be handled successfully by using constant step sizes and iterate averaging (state the order etc). This eliminates the fuzziness around the choice of step sizes.
\item The SA approach enable us to view TD algorithms as discretization of ODEs that converge to the PBE solution. Using this view, we also derive a new and stable TD algorithm.
\end{itemize}
We also present numerical examples along side the analysis.
\begin{comment}
\textbf{Model-free RL} algorithms learn the value function directly from the data without building an explicit model. A host of model free methods employ the idea of stochastic approximation (SA), wherein, noisy estimates from the samples are used in the place of true quantities. The name stochastic approximation signifies the fact that the noisy quantities are stochastically approximate and are equal to the true quantities only in the expectation. SA technique has been employed to a variety of scenarios including RL, stochastic optimization and online learning. The theory of SA deals with understanding and characterizing conditions under which SA algorithms are stable and convergent.\par
\textbf{Temporal Difference (TD)} learning algorithms form an important sub-class of RL algorithms. The TD family of algorithms are model-free and employ LFA when the number of states is large. Most TD algorithms (with a few exceptions) are also SA algorithms. 
The conditions for convergence vary widely across the family of TD algorithms. In particular, TD($\lambda$), Q-learning and SARSA are not true stochastic gradient algorithms, and as a result, are convergent only under restrictive conditions\cite{} and especially can be divergent in the off-policy setting. In order to address the off-policy learning, the GT of algorithms was introduced in \cite{}. Even though GTD is convergent in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent algorithm with respect to its original objective \cite{}. Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation. The LSTD family of algorithms are stable, nevertheless, are favourable because, they are based on matrix inversion and need a minimum of $O(n^2)$ computations per time step.
\textbf{Focus} of this paper is the TD family of algorithms. In particular, we are interested in those variants that make only $O(n)$ computations per time step (thus the LSTD variants are out of the scope of this paper). We analyze such algorithms using the theory of stochastic approximation. The motivation for our work stems from the following reasons
\begin{itemize}
\item The different TD variants either solve a full rank linear system in $n$ variables or minimize an objective function in $n$ variables. Typically, the updates are linear in the variables and converge to the appropriate solution. However, the algorithms are usually derived in an ad-hoc fashion.
\item Whether or not a given TD variant is a true stochastic gradient descent algorithm, it is nonetheless an SA algorithm. However, a unified analysis from this view point is absent in literature.
\item As with any SA algorithm, there is no clear step-size rule.
\item The standard SA theory deals with identifying an ordinary differential equation corresponding to a given SA algorithm. The properties of the algorithm are then studied by analyzing the ODE. Since, the updates of TD variants are linear, the corresponding ODEs will have a driving matrix. While it is clear that the rate of convergence of the ODE is affected by the spectral properties of the driving matrix, it is not clear how it affects the finite time performance of the algorithms.
\item Finite time performance of newer GTD algorithms have been presented in \cite{}. However, the bounds are using the bounds available for stochastic gradient descent in the most general setting. Since, these bounds do not exploit the linear nature of the updates, the bounds are likely to be too conservative.
\end{itemize}
In an attempt to address some the above concerns, we make the following contribution in this paper:
\begin{enumerate}
\item We use results of \cite{} for the linear least squares regression and observe that similar arguments can be made for the case of the TD family of algorithms. 
\item The analysis along \cite{} shows that the finite time performance has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples).
\item We show that the effect of the condition number of the driving matrix on the finite time performance can be handled successfully by using constant step sizes and iterate averaging. The analysis is simple, elegant, insightful and practical at the same time. Since, the analysis makes use of the linear nature of the updates, it leads to more realistic bounds than those in \cite{}.
\item The analysis provides a unified framework to compare all the TD variants.
\item As a result, of our analysis, we also present a newer and stable TD algorithm
\end{enumerate}
\textbf{Open Question} An upshot of our analysis is that, any TD algorithm can be viewed as a discretized version of a ODE that converges to the TD solution. This throws an open question as to how to design efficient ODEs given a linear system and how to discretize the same. While our paper presents a case for the importance of this open question, we are at very early stages of addressing this open question.
Secondly, in many cases, the agent needs to compute the value function of a policy (\emph{target}) different from the policy (\emph{behaviour}) that was used to collect the samples. This is known as \emph{off-policy} learning. The third issue is the \emph{curse-of-dimensionality} which denotes the fact that the number of states grow exponentially in the number of state variables. The value function of a policy can be computed by solving the Bellman equation (BE), which is a set $|S|$ of linear equations in $|S|$ variables.

\textbf{Open Question} An upshot of our analysis is that, any TD algorithm can be viewed as a discretized version of a ODE that converges to the TD solution. This throws an open question as to how to design efficient ODEs given a linear system and how to discretize the same. While our paper presents a case for the importance of this open question, we are at very early stages of addressing this open question.
\end{comment}