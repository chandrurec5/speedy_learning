%!TEX root =  speedylearn.tex
\section{Introduction}

The motivation for our study is the problem of \emph{value-function} estimation in reinforcement learning (RL), wherein we are required to estimate the value function corresponding to a Markov reward process.\footnote{Please refer \cite{BertB} for a detailed background.} A particular aspect of the RL setting is that the explicit model of the Markov reward process is not known and only data in the form of samples is made available. We consider linear value-function learning algorithms which are based on the idea of \emph{temporal difference} learning.
The term \emph{temporal difference} stands for the  error in the estimate of the value function obtained as a difference of terms involving the values in the successive states. The TD algorithms are LSA algorithms and are popular in applications where only $O(n)$ computations are desired per time step because perhaps $n$ (which is the number of features used to approximate the value function) is so large that only $O(n)$ methods are feasible. Stability and rate of convergence have been the two most important aspects that have been considered in the development of the various TD algorithms. 

A critical issue related the finite-time performance of value-function learning algorithms is the step-size rule which dictates both the stability of the algorithm as well as its rate of convergence. In this paper we are interested in understanding how the so-called Rupert-Polyak (RP) averaging method

We are interested in understanding the finite-time behaviour of the linear stochastic approximation (LSA) algorithm given by $x_{t+1}=x_t+\alpha_t(g_t-H_t)x_t$, where $x_t\in \R^n$ are the iterates, $\{\alpha_t\geq 0\}$ are step-sizes, $g_t\in \R^n$ and $H_t\in \R^{n\times n}$ are \emph{noisy} samples of a design vector $g\in\R^n$ and a design matrix $H\in\R^{n\times n}$ respectively, i.e., $\E[g_t]=g$ and $\E[H_t]=H$. The LSA algorithm is quite common in scenarios where only access to noisy samples of true quantities is available. In this paper, we intend to study the finite-time performance of the LSA algorithm.


Our work is inspired by a related work of \cite{bachaistats} who consider the problem of linear prediction under i.i.d. (independent, and identically distributed) data and squared expected loss. In this setting, a natural choice is to use a stochastic gradient descent (SGD) algorithm, which gives rise to an LSA algorithm. The issue with SGD is that under standard step-size choices, the error will be sensitive to the conditioning of the Grammian ($H$ with out notation) of the underlying linear least-squares problem. Using a constant step-size with iterate averaging, the so-called Rupert-Polyak (RP) averaging method, has been known to avoid this problem in the asymptotic regime.  \cite{bachaistats} revisit RP averaging to provide specific suggestions on selecting the constant step-size and demonstrate both empirically  \todoc{true?} and theoretically that the resulting method exhibits excellent, robust \emph{finite-time} performance. Their analysis splits the finite-time error into two terms namely the bias term (due to the initial condition) and the variance term (due to noise) and they show that, under mild assumptions on the data for the said constant step-size rule with iterate averaging a finite-time performance of $O(1/t^2)$ and $O(1/t)$ respectively for the bias and the variance terms can be achieved regardless of the conditioning of $H$. \todoc{Is this true?} 

Our goal is to study whether their results and observation transfer to the RL setting, where the important difference to the least-squares setting is that in RL the matrices $H/H_t$ are not symmetric. We nevertheless show that analysis on the lines of \cite{bachaistats} still holds for the LSAs in the RL setting, thereby presenting an analysis under more generalized assumptions i.e., in the absence of symmetry. \todoc{Can we say something about the proof technique? We employ the same ideas, we change something, simplify the derivation, etc?}

 
While almost all the TD algorithms of interest make use of linear updates, the design of these update rules are ad-hoc \cite{gtdfinite}, thereby lacking a systematic methodology. Our work presents a refined view of the TD algorithms on two accounts. Firstly, our analysis show that most of the TD algorithms achieve rates of convergence faster than previously believed. Secondly, we show that the choice of the update rule can be appreciated better by looking at the spectral properties of the corresponding design matrix. The analysis shows that there is no clear winner amongst the various TD algorithms and different algorithms can be better for different Markov reward processes.\par
The specific contributions in this paper are listed as below
\begin{itemize}[leftmargin=*] 
\item We show (in \Cref{main result}) that LSA algorithms with constant step-size rule and RP averaging achieve $O(1/t^2)$ for forgetting the initial bias and $O(1/t)$ rate (\Cref{maintheorem}) for the variance.
\item We show specific counter examples (\Cref{opti}) to refute the conjecture in \cite{bachaistats} on the maximum allowable constant step-size for LSA algorithms.
\item We discuss how the performance of the various TD algorithms can be understood by analyzing the spectral properties of the design matrix. For the purpose of illustrating our view, we introduce (\Cref{design}) and analyze certain newer TD variants.
\end{itemize}
We also present numerical examples along side the analysis.
