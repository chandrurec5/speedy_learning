\section{Introduction}
\textbf{Reinforcement Learning} is a paradigm where the learning algorithm/agent needs to learn via direct interaction with the environment. In most cases, the underlying environment can be modelled as a Markov Decision Process (MDP). At time any given time instant, the agent knows the state (described via the state variables) of the environment (i.e., the MDP) and has to choose an action from a given action set. The action selection mechanism of the agent is formally known as the policy, and each policy is associated with a value function, which is a mapping from the set of states to reals that specifies the value obtained by following a given policy starting from a given state. Typically, in order to improve a given policy, its value function needs to be computed first. Thus learning the value function of a given policy assumes centre stage.\par
\textbf{Algorithms} that learn the value function need to address two important issues namely lack of explicit model information (of the underlying MDP) and dimensionality of the state space. It is known that the value function of a policy can be computed by solving a $|S|\times|S|$ (where $|S|$ is the cardinality of the state space) linear system known as the Bellman equation (BE). However, the coefficients of the BE are described in terms of the model parameters, which are not known explicitly in an RL setting. Due to the \emph{curse-of-dimensionality} $|S|$ grows exponentially in the number of state variables. As a result, when $|S|$ is very large, it is infeasible to store, retrieve and compute the value for each and every state.\par
\textbf{Linear Function Approximation} (LFA) is a common dimensionality reduction approach employed to reduce the computational overhead when $|S|$ is large. In this approach, the value function is approximated by a linear combination of $n$ ($<<|S|$) basis functions. The problem now boils down to computing a weight vector that specifies the weights of different basis functions in the linear combination. Usually, once the basis is chosen, the weight vector can computed by solving a reduced ($n\times n$) linear system known as the projected Bellman equation (PBE). It is important to note that the coefficients of the PBE are dependent not only on the basis functions but also on the model parameters. Thus, RL algorithms need to compute the PBE solution using only the samples obtained via direct interaction.\par
The \emph{temporal difference} (TD) family of learning algorithms are a very important sub-class of RL algorithms and compute the solution to the PBE. The term \emph{temporal difference} stands for the  error in the estimate of the value function obtained as a difference of terms involving the values at successive states. Since, the coefficients of the PBE are not known, the TD algorithms make use of the temporal difference to take  incremental steps at each iteration. The direction and magnitude of the incremental step varies across the various TD algorithms. Thus, it then becomes important to ensure that the weight vector indeed converges to the PBE solution. Since most TD algorithms update only the weight vector, the computational complexity is only $O(n)$.
\begin{comment}
%The first TD algorithm (i.e., TD($0$)) was introduced in \cite{} and TD with LFA was analyzed in \cite{}. 
%The TD family includes several variants such as TD such as TD($\lambda$), SARSA, Q-learning, LSTD, iLSTD and Gradient-TD (GTD) \cite{}. 
%Aspects such as computational complexity and stability have driven the course of development of these various TD algorithms. Most of the TD algorithms perform $O(n)$ computations per time step and are %suitable for a variety of scenarios. 
The issue with TD($0$) is that it makes use of only the current sample and is not sample efficient. This issue was alleviated by the LTSD by making use of all the data to estimate and solve an empirical PBE. Since LSTD involves matrix updates and inversions at each step its computational complexity is minimum $O(n^2)$. In contrast, since simple TD($0$) involves only $O(n)$ computations per step, it continues to be of interest in applications in which cannot afford $O(n^2)$ required by LSTD.\par
\end{comment}
\tdo, Q-learning and SARSA algorithms are not stable in the most general setting, especially the \emph{off-policy} case. Here, the policy (\emph{target}) whose value function needs to be  computed which is different from the policy (\emph{behaviour}) that was used to collect the samples. The instability arises due to the fact that TD, SARSA and Q-learning are not true gradient algorithms. The Gradient TD (GTD) family of algorithms perform updates in the gradient direction and are stable. Even though GTD is stable in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent algorithm with respect to its original objective \cite{}. Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation.\par
Another related and important aspect of concern is the speed of convergence. A typical step-size rule is to choose them to diminish at a rate of $O(1/t)$ with respect to time. However, such step-sizes might be problem sensitive and can lead to poor speed of convergence. One remedy in the literature was to use the idea of speeding TD learning was used in speedy Q-learning (SQL) \cite{sql}. \cite{} uses adaptive step sizes based on the TD error to achieve faster learning rates. Recent RL algorithms such as GTD-MP claim to make use of ideas of accelerated gradient descent to achieve increase speed of convergence.\par
\textbf{Motivation} While the TD methods have been developed to address one issue (computational/speed/stability) at a time, this approach has led to ad-hoc derivation of the updates, speeding techniques and step-size rules, thus does not provide a complete holistic picture. The focus of this paper is to study these TD algorithms in a more principled and unified manner using the theory of stochastic approximation (SA) algorithms. The term stochastic approximation signifies the fact that the noisy quantities used in the algorithms are stochastically approximate, and are equal to the quantities only in the expectation. Since the TD algorithms are also based on such stochastic updates, we make use of recent finite time results in SA \cite{} to argue our case. The specific contributions in this paper are listed as below
\begin{itemize}[leftmargin=*] 
\item We use of the standard result in SA theory to identify ordinary differential equations related to each TD algorithm. Thus each of the different TD algorithms can be viewed as a discretization of its corresponding ODE. While all the ODEs converge to the PBE solution, their dynamics is dictated by different design matrices. Thus choosing design matrices that lead to stable ODEs is key.
\item The finite time performance of any TD algorithm has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples). Further, we then argue on the lines of \cite{} to show that using conventional step size rules that diminish in $O(1/t)$ are sensitive to the spectral properties of the design matrix.
\item We then show that constant step size with Rupert-Polyak averaging of iterates yields $O(1/t^2)$ for forgetting initial conditions and $O(1/n)$ rate for variance term.
\item The expression for finite time performance (initial condition and bias) involves constant terms which depend on the spectral properties of the design matrix. For instance, the `slownessâ€™ of GTD can be attributed to its design matrix.
\item Our SA based approach is not only useful for analysis but also for synthesis of RL algorithms. For instance, we show that the GTP-MP algorithm can be derived as a predictor-corrector discretization of the ODE corresponding to the GTD algorithm. We add more weight to this argument by deriving a new stable TD method by modifying the predictor-corrector algorithm.
\end{itemize}
We also present numerical examples along side the analysis.
\begin{comment}
\textbf{Model-free RL} algorithms learn the value function directly from the data without building an explicit model. A host of model free methods employ the idea of stochastic approximation (SA), wherein, noisy estimates from the samples are used in the place of true quantities. The name stochastic approximation signifies the fact that the noisy quantities are stochastically approximate and are equal to the true quantities only in the expectation. SA technique has been employed to a variety of scenarios including RL, stochastic optimization and online learning. The theory of SA deals with understanding and characterizing conditions under which SA algorithms are stable and convergent.\par
\textbf{Temporal Difference (TD)} learning algorithms form an important sub-class of RL algorithms. The TD family of algorithms are model-free and employ LFA when the number of states is large. Most TD algorithms (with a few exceptions) are also SA algorithms. 
The conditions for convergence vary widely across the family of TD algorithms. In particular, TD($\lambda$), Q-learning and SARSA are not true stochastic gradient algorithms, and as a result, are convergent only under restrictive conditions\cite{} and especially can be divergent in the off-policy setting. In order to address the off-policy learning, the GT of algorithms was introduced in \cite{}. Even though GTD is convergent in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent algorithm with respect to its original objective \cite{}. Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation. The LSTD family of algorithms are stable, nevertheless, are favourable because, they are based on matrix inversion and need a minimum of $O(n^2)$ computations per time step.
\textbf{Focus} of this paper is the TD family of algorithms. In particular, we are interested in those variants that make only $O(n)$ computations per time step (thus the LSTD variants are out of the scope of this paper). We analyze such algorithms using the theory of stochastic approximation. The motivation for our work stems from the following reasons
\begin{itemize}[leftmargin=*]
\item The different TD variants either solve a full rank linear system in $n$ variables or minimize an objective function in $n$ variables. Typically, the updates are linear in the variables and converge to the appropriate solution. However, the algorithms are usually derived in an ad-hoc fashion.
\item Whether or not a given TD variant is a true stochastic gradient descent algorithm, it is nonetheless an SA algorithm. However, a unified analysis from this view point is absent in literature.
\item As with any SA algorithm, there is no clear step-size rule.
\item The standard SA theory deals with identifying an ordinary differential equation corresponding to a given SA algorithm. The properties of the algorithm are then studied by analyzing the ODE. Since, the updates of TD variants are linear, the corresponding ODEs will have a driving matrix. While it is clear that the rate of convergence of the ODE is affected by the spectral properties of the driving matrix, it is not clear how it affects the finite time performance of the algorithms.
\item Finite time performance of newer GTD algorithms have been presented in \cite{}. However, the bounds are using the bounds available for stochastic gradient descent in the most general setting. Since, these bounds do not exploit the linear nature of the updates, the bounds are likely to be too conservative.
\end{itemize}
In an attempt to address some the above concerns, we make the following contribution in this paper:
\begin{enumerate}
\item We use results of \cite{} for the linear least squares regression and observe that similar arguments can be made for the case of the TD family of algorithms. 
\item The analysis along \cite{} shows that the finite time performance has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples).
\item We show that the effect of the condition number of the driving matrix on the finite time performance can be handled successfully by using constant step sizes and iterate averaging. The analysis is simple, elegant, insightful and practical at the same time. Since, the analysis makes use of the linear nature of the updates, it leads to more realistic bounds than those in \cite{}.
\item The analysis provides a unified framework to compare all the TD variants.
\item As a result, of our analysis, we also present a newer and stable TD algorithm
\end{enumerate}
\textbf{Open Question} An upshot of our analysis is that, any TD algorithm can be viewed as a discretized version of a ODE that converges to the TD solution. This throws an open question as to how to design efficient ODEs given a linear system and how to discretize the same. While our paper presents a case for the importance of this open question, we are at very early stages of addressing this open question.
Secondly, in many cases, the agent needs to compute the value function of a policy (\emph{target}) different from the policy (\emph{behaviour}) that was used to collect the samples. This is known as \emph{off-policy} learning. The third issue is the \emph{curse-of-dimensionality} which denotes the fact that the number of states grow exponentially in the number of state variables. The value function of a policy can be computed by solving the Bellman equation (BE), which is a set $|S|$ of linear equations in $|S|$ variables.

\textbf{Open Question} An upshot of our analysis is that, any TD algorithm can be viewed as a discretized version of a ODE that converges to the TD solution. This throws an open question as to how to design efficient ODEs given a linear system and how to discretize the same. While our paper presents a case for the importance of this open question, we are at very early stages of addressing this open question.
\end{comment}