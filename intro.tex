\section{Introduction}
Reinforcement Learning (RL) is a paradigm where the learning algorithm/agent needs to learn via direct interaction with the environment. In most cases, the underlying environment can be modelled as a Markov Decision Process (MDP). At any given time instant, the agent knows the state (described via the state variables) of the environment (i.e., the MDP) and has to choose an action from a given action set $A$. The action selection mechanism of the agent is formally known as the policy, and each policy is associated with a value function, which is a mapping from the set of states $S$ to the reals $\R$ that specifies the value obtained by following a given policy starting from a given state. Typically, in order to improve a given policy, its value function needs to be computed first. Thus learning the value function of a given policy is an important sub-problem in RL.\par
RL algorithms that learn the value function need to address two important issues namely, the lack of explicit model information (of the underlying MDP) and the dimensionality of the state space. It is known that the value function of a policy can be computed by solving a $|S|\times|S|$ (where $|S|$ is the cardinality of the state space) system of linear equations known as the Bellman equation (BE). However, the coefficients of the BE are described in terms of the model parameters, which are not known explicitly in an RL setting. Due to the \emph{curse-of-dimensionality} $|S|$ grows exponentially in the number of state variables. As a result, when $|S|$ is very large, it is infeasible to store, retrieve and compute the value for each and every state.\par
Linear Function Approximation (LFA) is a common dimensionality reduction approach employed to reduce the computational overhead when $|S|$ is large. In this approach, the value function is approximated by a linear combination of $n$ basis functions (with $n<<|S|$). The problem now boils down to computing a weight vector that specifies the weights of the different basis functions in the linear combination. Usually, once the basis is chosen, the weight vector can computed by solving a reduced ($n\times n$) linear system known as the projected Bellman equation (PBE). It is important to note that the coefficients of the PBE are dependent not only on the basis functions but also on the model parameters. Thus, RL algorithms need to compute the PBE solution using only the samples obtained via direct interaction.\par
The \emph{temporal difference} (TD) family of RL algorithms are value function learning algorithms the compute the PBE solution using the sampled data. The term \emph{temporal difference} stands for the  error in the estimate of the value function obtained as a difference of terms involving the values in the successive states. The TD algorithms typically employ LFA and use the temporal difference to make incremental updates to the weight vector at each time step. It takes only $O(n)$ computations per step to update the weight vector, an aspect that makes TD algorithms desirable in many practical domains \cite{}. The direction and magnitude of the incremental update varies across the various TD algorithms, and it then becomes important to ensure that the weight vector indeed converges to the PBE solution. 
\begin{comment}
%The first TD algorithm (i.e., TD($0$)) was introduced in \cite{} and TD with LFA was analyzed in \cite{}. 
%The TD family includes several variants such as TD such as TD($\lambda$), SARSA, Q-learning, LSTD, iLSTD and Gradient-TD (GTD) \cite{}. 
%Aspects such as computational complexity and stability have driven the course of development of these various TD algorithms. Most of the TD algorithms perform $O(n)$ computations per time step and are %suitable for a variety of scenarios. 
The issue with TD($0$) is that it makes use of only the current sample and is not sample efficient. This issue was alleviated by the LTSD by making use of all the data to estimate and solve an empirical PBE. Since LSTD involves matrix updates and inversions at each step its computational complexity is minimum $O(n^2)$. In contrast, since simple TD($0$) involves only $O(n)$ computations per step, it continues to be of interest in applications in which cannot afford $O(n^2)$ required by LSTD.\par
\end{comment}
Stability and rate of convergence are two important aspects that have driven the course of development of the various TD algorithms. \tdo, Q-learning and SARSA algorithms are not stable in the \emph{off-policy} setting, wherein, the policy (\emph{target}) whose value function needs to be  computed which is different from the policy (\emph{behaviour}) that was used to collect the samples. The instability arises due to the fact that TD, SARSA and Q-learning are not true gradient algorithms. In order to alleviate the instability, the Gradient-TD (GTD)  algorithm was first introduced in \cite{}. The GTD algorithm performs updates in the gradient direction and is stable. Two other stable variants of the GTD namely GTD2 and TDC were introduced in \cite{}.\par
Rate of convergence of the state-of-t
Even though GTD is stable in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent (SGD) algorithm with respect to its original objective \cite{}.  Thus standard results available in the literature for the rate of convergence of SGD algorithms do no Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation.\par
\begin{comment}
Another related and important aspect of concern is the speed of convergence. A typical step-size rule is to choose them to diminish at a rate of $O(1/t)$ with respect to time. However, such step-sizes might be problem sensitive and can lead to poor speed of convergence. One remedy in the literature was to use the idea of speeding TD learning was used in speedy Q-learning (SQL) \cite{sql}. \cite{} uses adaptive step sizes based on the TD error to achieve faster learning rates. Recent RL algorithms such as GTD-MP claim to make use of ideas of accelerated gradient descent to achieve increase speed of convergence.\par
\end{comment}
\textbf{Motivation} While the TD methods have been developed to address one issue (computational/speed/stability) at a time, this approach has led to ad-hoc derivation of the updates, speeding techniques and step-size rules, thus does not provide a complete holistic picture. The focus of this paper is to study these TD algorithms in a more principled and unified manner using the theory of stochastic approximation (SA) algorithms. The term stochastic approximation signifies the fact that the noisy quantities used in the algorithms are stochastically approximate, and are equal to the quantities only in the expectation. Since the TD algorithms are also based on such stochastic updates, we make use of recent finite time results in SA \cite{} to argue our case. The specific contributions in this paper are listed as below
\begin{itemize}[leftmargin=*] 
\item We use of the standard result in SA theory to identify ordinary differential equations related to each TD algorithm. Thus each of the different TD algorithms can be viewed as a discretization of its corresponding ODE. While all the ODEs converge to the PBE solution, their dynamics is dictated by different design matrices. Thus choosing design matrices that lead to stable ODEs is key.
\item The finite time performance of any TD algorithm has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples). Further, we then argue on the lines of \cite{} to show that using conventional step size rules that diminish in $O(1/t)$ are sensitive to the spectral properties of the design matrix.
\item We then show that constant step size with Rupert-Polyak averaging of iterates yields $O(1/t^2)$ for forgetting initial conditions and $O(1/n)$ rate for variance term.
\item The expression for finite time performance (initial condition and bias) involves constant terms which depend on the spectral properties of the design matrix. For instance, the `slownessâ€™ of GTD can be attributed to its design matrix.
\end{itemize}
We also present numerical examples along side the analysis.
