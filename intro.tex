\section{Introduction}
\textbf{Reinforcement Learning} is a paradigm where the learning algorithm/agent needs to learn via direct interaction with the environment. In most cases, the underlying environment can be modelled as a Markov Decision Process (MDP). At time any given time instant, the agent knows the state (described via the state variables) of the environment (i.e., the MDP) and has to choose an action from a given action set. The action selection mechanism of the agent is formally known as the policy, and each policy is associated with a value function, which is a mapping from the set of states to reals that specifies the value obtained by following a given policy starting from a given state. Typically, in order to improve a given policy, its value function needs to be computed first. Thus learning the value function of a given policy assumes centre stage.\par
\textbf{Algorithms} that learn the value function need to address two important issues namely lack of explicit model information (of the underlying MDP) and dimensionality of the state space. It is known that the value function of a policy can be computed by solving a $|S|\times|S|$ (where $|S|$ is the cardinality of the state space) linear system known as the Bellman equation (BE). However, the coefficients of the BE are described in terms of the model parameters, which are not known explicitly in an RL setting. Due to the \emph{curse-of-dimensionality} $|S|$ grows exponentially in the number of state variables. As a result, when $|S|$ is very large, it is infeasible to store, retrieve and compute the value for each and every state.\par
\textbf{Linear Function Approximation} (LFA) is a common dimensionality reduction approach employed to reduce the computational overhead when $|S|$ is large. In this approach, the value function is approximated by a linear combination of $n$ ($<<|S|$) basis functions. The problem now boils down to computing a weight vector that specifies the weights of different basis functions in the linear combination. Usually, once the basis is chosen, the weight vector can computed by solving a reduced ($n\times n$) linear system known as the projected Bellman equation (PBE). It is important to note that the coefficients of the PBE are dependent not only on the basis functions but also on the model parameters. Thus, RL algorithms need to compute the PBE solution using only the samples obtained via direct interaction.\par
The \emph{temporal difference} (TD) family of learning algorithms are a very important sub-class of RL algorithms and compute the solution to the PBE. The term \emph{temporal difference} stands for the  error in the estimate of the value function obtained as a difference of terms involving the values at successive states. Since, the coefficients of the PBE are not known, the TD algorithms make use of the temporal difference to take  incremental steps at each iteration. The direction and magnitude of the incremental step varies across the various TD algorithms. Thus, it then becomes important to ensure that the weight vector indeed converges to the PBE solution. Since most TD algorithms update only the weight vector, the computational complexity is only $O(n)$.
\begin{comment}
%The first TD algorithm (i.e., TD($0$)) was introduced in \cite{} and TD with LFA was analyzed in \cite{}. 
%The TD family includes several variants such as TD such as TD($\lambda$), SARSA, Q-learning, LSTD, iLSTD and Gradient-TD (GTD) \cite{}. 
%Aspects such as computational complexity and stability have driven the course of development of these various TD algorithms. Most of the TD algorithms perform $O(n)$ computations per time step and are %suitable for a variety of scenarios. 
The issue with TD($0$) is that it makes use of only the current sample and is not sample efficient. This issue was alleviated by the LTSD by making use of all the data to estimate and solve an empirical PBE. Since LSTD involves matrix updates and inversions at each step its computational complexity is minimum $O(n^2)$. In contrast, since simple TD($0$) involves only $O(n)$ computations per step, it continues to be of interest in applications in which cannot afford $O(n^2)$ required by LSTD.\par
\end{comment}
\tdo, Q-learning and SARSA algorithms are not stable in the most general setting, especially the \emph{off-policy} case. Here, the policy (\emph{target}) whose value function needs to be  computed which is different from the policy (\emph{behaviour}) that was used to collect the samples. The instability arises due to the fact that TD, SARSA and Q-learning are not true gradient algorithms. The Gradient TD (GTD) family of algorithms perform updates in the gradient direction and are stable. Even though GTD is stable in the off-policy case, it was pointed out in \cite{}, that even the GTD is not a true stochastic gradient descent algorithm with respect to its original objective \cite{}. Authors in \cite{} propose newer variants of the GTD and show that they are true gradient algorithms, however, with respect to a different objective function obtained via a primal-dual saddle point formulation.\par
\begin{comment}
Another related and important aspect of concern is the speed of convergence. A typical step-size rule is to choose them to diminish at a rate of $O(1/t)$ with respect to time. However, such step-sizes might be problem sensitive and can lead to poor speed of convergence. One remedy in the literature was to use the idea of speeding TD learning was used in speedy Q-learning (SQL) \cite{sql}. \cite{} uses adaptive step sizes based on the TD error to achieve faster learning rates. Recent RL algorithms such as GTD-MP claim to make use of ideas of accelerated gradient descent to achieve increase speed of convergence.\par
\end{comment}
\textbf{Motivation} While the TD methods have been developed to address one issue (computational/speed/stability) at a time, this approach has led to ad-hoc derivation of the updates, speeding techniques and step-size rules, thus does not provide a complete holistic picture. The focus of this paper is to study these TD algorithms in a more principled and unified manner using the theory of stochastic approximation (SA) algorithms. The term stochastic approximation signifies the fact that the noisy quantities used in the algorithms are stochastically approximate, and are equal to the quantities only in the expectation. Since the TD algorithms are also based on such stochastic updates, we make use of recent finite time results in SA \cite{} to argue our case. The specific contributions in this paper are listed as below
\begin{itemize}[leftmargin=*] 
\item We use of the standard result in SA theory to identify ordinary differential equations related to each TD algorithm. Thus each of the different TD algorithms can be viewed as a discretization of its corresponding ODE. While all the ODEs converge to the PBE solution, their dynamics is dictated by different design matrices. Thus choosing design matrices that lead to stable ODEs is key.
\item The finite time performance of any TD algorithm has two terms namely the bias term (due to the initial conditions) and the variance terms (due to the noisy samples). Further, we then argue on the lines of \cite{} to show that using conventional step size rules that diminish in $O(1/t)$ are sensitive to the spectral properties of the design matrix.
\item We then show that constant step size with Rupert-Polyak averaging of iterates yields $O(1/t^2)$ for forgetting initial conditions and $O(1/n)$ rate for variance term.
\item The expression for finite time performance (initial condition and bias) involves constant terms which depend on the spectral properties of the design matrix. For instance, the `slownessâ€™ of GTD can be attributed to its design matrix.
\end{itemize}
We also present numerical examples along side the analysis.
