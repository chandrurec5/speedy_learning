\section{Comparison}
\textbf{Challenges in Finite Time Analysis:} \cite{} points out that state-of-art (SOA) TD algorithms such as GTD, GTD-2 are not true stochastic gradient descent (SGD) algorithms (with respect to the loss function they aim to minimize), in that the update equations of these SOA algorithms, do not involve a gradient expressed as a single expectation. As a result, the finite sample analysis available for stochastic gradients are not applicable directly to the SOA TD algorithm. In order to fix this, \cite{} shows these methods are true SGD with respect to a primal-dual objective and make use of \cite{} with specific conditions on step-sizes to derive a finite time performance bound that scales as $O(\frac{1}{\sqrt{t}})$.\par
\begin{comment}
\textbf{Adaptive Step-sizes and Speed-up terms:} Efforts have been made to improve the performance of TD algorithms, by changing the step-sizes adaptively \cite{} or adding speed-up terms \cite{}. Whilst adaptive step-sizes can be guaranteed to improve, their effect on the finite time performance is not known. In the case of full state representation (tabular representation), the speedy Q-learning (SQL) algorithm adds a correction/speed-up term to achieve an $O(\frac{1}{\sqrt{t}})$ finite time performance. The analysis in this paper shows that the idea used by SQL cannot be extended effectively to the case when features are used.\par
\end{comment}
Asymptotic analysis of stochastic approximation (SA) algorithms involves studying the properties of an associated ordinary differential equation (ODE), and the different TD algorithms can be viewed as a different ways of discretizing the ODEs. The analysis in this paper hinges on the fact that whilst the SOA TD algorithms are not true gradient functions, they are nevertheless, LSA algorithms. Hence, our closely resembles the analysis of stochastic gradient descent (SGD) algorithm to solve the averaged least-mean-squares (ALMS) problem in \cite{bachalms}. In the ALMS problem, the data is presented in the form of $(X_i,Y_i),i=1\ldots,N$, where $X_i\in \R^n$ and $Y_i\in R$ and the aim is to compute $w^*\in \R^n$ such that $w^*=\arg\min_{w\in \R^n}\sum_{i=1}^N\parallel X_iw-Y_i\parallel^2$. In addition to condition~\ref{pd} in \Cref{lsaassump}, the ALMS problem also enjoys further structure in that the $H_t$ are symmetric matrices. \cite{} uses this structure to define linear symmetric operators on the space of symmetric matrices in order to provide tighter bounds on the step-sizes. In the case of TD algorithms, $H_t$ are not symmetric in general and hence the tighter analysis of \cite{} does not hold in this case. Nevertheless, we show that simpler analysis on the lines of \cite{} is possible for LSA algorithms in general under condition \Cref{lsaassump}. Our analysis shows that it is possible to use constant step sizes for SOA TD algorithms, and a finite time performance which scales $O(\frac{1}{t^2})$ for the \emph{bias} term and $O(\frac{1}{t})$ for the variance term can be achieved.
